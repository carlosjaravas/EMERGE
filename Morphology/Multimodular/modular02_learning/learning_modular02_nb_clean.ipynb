{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, pickle, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint as ri\n",
    "from random import uniform as ru\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.losses import Huber\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the training data into X,y datasets\n",
    "def load_dataset(scene_in = \"modular02a\"):\n",
    "    # Defining usefull variables\n",
    "    path = os.getcwd()\n",
    "    file_list = os.listdir(path)\n",
    "    scene_files_list = [item for item in file_list if scene_in in item and \"pkl\" in item]\n",
    "\n",
    "\n",
    "    file = open(path + \"\\\\\" + scene_files_list[0], \"rb\")\n",
    "    training_data = pickle.load(file)\n",
    "    training_df = pd.DataFrame(training_data)\n",
    "\n",
    "    #List to store the name for every joint data column\n",
    "    increments_columns = []\n",
    "    prev_j_positions_columns = []\n",
    "    post_j_positions_columns = []\n",
    "\n",
    "    #Creates the name for every column\n",
    "    num_joints = len(training_data[-1][\"increments\"])\n",
    "    for joint_n in range(num_joints):\n",
    "        #List to split inputs per joint\n",
    "        joint_inc_col_name = \"increments_\" + str(joint_n)\n",
    "        increments_columns.append(joint_inc_col_name)\n",
    "\n",
    "        prev_joint_pos_col_name = \"prev_j_positions_\" + str(joint_n)\n",
    "        prev_j_positions_columns.append(prev_joint_pos_col_name)\n",
    "\n",
    "        #List to split outputs per joint\n",
    "        post_joint_pos_col_name = \"post_j_positions_\" + str(joint_n)\n",
    "        post_j_positions_columns.append(post_joint_pos_col_name)\n",
    "\n",
    "\n",
    "    #Input columns per joint\n",
    "    increments_df = pd.DataFrame(training_df['increments'].to_list())\n",
    "    increments_df.columns = increments_columns\n",
    "\n",
    "\n",
    "    prev_j_positions_df = pd.DataFrame(training_df['prev_j_positions'].to_list())\n",
    "    prev_j_positions_df.columns = prev_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the X dataframe\n",
    "    X_df = pd.concat([increments_df, prev_j_positions_df, \n",
    "                    training_df[\"prev_pos_x\"], training_df[\"prev_pos_y\"], training_df[\"prev_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "\n",
    "    #Output columns per joint\n",
    "    post_j_positions_df = pd.DataFrame(training_df['post_j_positions'].to_list())\n",
    "    post_j_positions_df.columns = post_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the y dataframe\n",
    "    y_df = pd.concat([post_j_positions_df, \n",
    "                    training_df[\"post_pos_x\"], training_df[\"post_pos_y\"], training_df[\"post_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "    return(X_df,y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_X_y(X_a,y_a):\n",
    "    X_a_cols = ['current_state_x', 'current_state_y', 'current_state_z', 'joint0_current_state_rad', 'joint1_current_state_rad', 'joint0_actions_rad', 'joint1_actions_rad']\n",
    "\n",
    "    X_a = pd.concat([X_a[\"prev_pos_x\"], X_a[\"prev_pos_y\"] , X_a[\"prev_pos_z\"],\n",
    "                X_a[\"prev_j_positions_0\"], X_a[\"prev_j_positions_1\"],\n",
    "                X_a[\"increments_0\"], X_a[\"increments_1\"]], \n",
    "                        axis=\"columns\")\n",
    "\n",
    "    X_a.columns = X_a_cols\n",
    "\n",
    "    y_a_cols = ['future_state_x', 'future_state_y', 'future_state_z', 'joint0_future_state_rad', 'joint1_future_state_rad']\n",
    "\n",
    "    y_a = pd.concat([y_a[\"post_pos_x\"], y_a[\"post_pos_y\"] , y_a[\"post_pos_z\"],\n",
    "                y_a[\"post_j_positions_0\"], y_a[\"post_j_positions_1\"]], \n",
    "                        axis=\"columns\")\n",
    "    y_a.columns = y_a_cols\n",
    "    return X_a, y_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modular 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"modular02_\"\n",
    "X, y =load_dataset(scene)\n",
    "X, y = adapt_X_y(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separan los datos de entrenmiento (80%) y los de prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling de los datos \n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0268 - val_loss: 0.0017\n",
      "Epoch 2/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 8.9532e-04 - val_loss: 3.8412e-04\n",
      "Epoch 3/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.5123e-04 - val_loss: 1.6989e-04\n",
      "Epoch 4/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.4219e-04 - val_loss: 1.1901e-04\n",
      "Epoch 5/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.0752e-04 - val_loss: 1.0169e-04\n",
      "Epoch 6/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 8.9917e-05 - val_loss: 8.6052e-05\n",
      "Epoch 7/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 7.7599e-05 - val_loss: 6.5175e-05\n",
      "Epoch 8/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.7622e-05 - val_loss: 8.1524e-05\n",
      "Epoch 9/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.2695e-05 - val_loss: 6.2157e-05\n",
      "Epoch 10/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.7810e-05 - val_loss: 4.8159e-05\n",
      "Epoch 11/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.2546e-05 - val_loss: 4.3731e-05\n",
      "Epoch 12/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.9808e-05 - val_loss: 3.9487e-05\n",
      "Epoch 13/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.5655e-05 - val_loss: 3.9123e-05\n",
      "Epoch 14/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.3573e-05 - val_loss: 4.2714e-05\n",
      "Epoch 15/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.1321e-05 - val_loss: 3.7106e-05\n",
      "Epoch 16/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.9266e-05 - val_loss: 3.5392e-05\n",
      "Epoch 17/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.7892e-05 - val_loss: 3.2181e-05\n",
      "Epoch 18/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.6113e-05 - val_loss: 3.4557e-05\n",
      "Epoch 19/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.4037e-05 - val_loss: 2.7921e-05\n",
      "Epoch 20/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.3473e-05 - val_loss: 3.9431e-05\n",
      "Epoch 21/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.2947e-05 - val_loss: 2.7420e-05\n",
      "Epoch 22/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.1265e-05 - val_loss: 3.5710e-05\n",
      "Epoch 23/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.9653e-05 - val_loss: 2.8720e-05\n",
      "Epoch 24/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.0130e-05 - val_loss: 2.6705e-05\n",
      "Epoch 25/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.9247e-05 - val_loss: 2.6954e-05\n"
     ]
    }
   ],
   "source": [
    "# Creacion y entrenamiento del modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada con 22 unidades (correspondiente al número de entradas)\n",
    "model.add(Dense(units=7, input_dim=7, activation='relu'))\n",
    "\n",
    "# Capas ocultas con 64 unidades cada una\n",
    "model.add(Dense(units=24, activation='relu'))\n",
    "\n",
    "# Capa de salida con 14 unidades (correspondiente al número de salidas) y activación lineal\n",
    "model.add(Dense(units=5, activation='linear'))\n",
    "\n",
    "# Compilar el modelo con una función de pérdida adecuada para regresión\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=1.0))\n",
    "\n",
    "# Supongamos que X_train y y_train son tus datos de entrenamiento\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_\" + scene + \".keras\")\n",
    "with open(\"scaler_\" + scene + '.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "with open(\"model_\" + scene + '.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the training data into X,y datasets\n",
    "def load_dataset(scene_in = \"modular02a\", data_path = \"\\\\training_data\\\\2023_14_11\"):\n",
    "    # Defining usefull variables\n",
    "    path = os.getcwd() + data_path\n",
    "    print(path)\n",
    "    file_list = os.listdir(path)\n",
    "    scene_files_list = [item for item in file_list if scene_in in item and \"pkl\" in item]\n",
    "\n",
    "\n",
    "    file = open(path + \"\\\\\" + scene_files_list[0], \"rb\")\n",
    "    training_data = pickle.load(file)\n",
    "    training_df = pd.DataFrame(training_data)\n",
    "\n",
    "    #List to store the name for every joint data column\n",
    "    increments_columns = []\n",
    "    prev_j_positions_columns = []\n",
    "    post_j_positions_columns = []\n",
    "\n",
    "    #Creates the name for every column\n",
    "    num_joints = len(training_data[-1][\"increments\"])\n",
    "    for joint_n in range(num_joints):\n",
    "        #List to split inputs per joint\n",
    "        joint_inc_col_name = \"increments_\" + str(joint_n)\n",
    "        increments_columns.append(joint_inc_col_name)\n",
    "\n",
    "        prev_joint_pos_col_name = \"prev_j_positions_\" + str(joint_n)\n",
    "        prev_j_positions_columns.append(prev_joint_pos_col_name)\n",
    "\n",
    "        #List to split outputs per joint\n",
    "        post_joint_pos_col_name = \"post_j_positions_\" + str(joint_n)\n",
    "        post_j_positions_columns.append(post_joint_pos_col_name)\n",
    "\n",
    "\n",
    "    #Input columns per joint\n",
    "    increments_df = pd.DataFrame(training_df['increments'].to_list())\n",
    "    increments_df.columns = increments_columns\n",
    "\n",
    "\n",
    "    prev_j_positions_df = pd.DataFrame(training_df['prev_j_positions'].to_list())\n",
    "    prev_j_positions_df.columns = prev_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the X dataframe\n",
    "    X_df = pd.concat([increments_df, prev_j_positions_df, \n",
    "                    training_df[\"prev_pos_x\"], training_df[\"prev_pos_y\"], training_df[\"prev_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "\n",
    "    #Output columns per joint\n",
    "    post_j_positions_df = pd.DataFrame(training_df['post_j_positions'].to_list())\n",
    "    post_j_positions_df.columns = post_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the y dataframe\n",
    "    y_df = pd.concat([post_j_positions_df, \n",
    "                    training_df[\"post_pos_x\"], training_df[\"post_pos_y\"], training_df[\"post_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "    return(X_df,y_df)\n",
    "\n",
    "def adapt_X_y(X_a,y_a):\n",
    "    X_a_cols = ['current_state_x', 'current_state_y', 'current_state_z', 'joint0_current_state_rad', 'joint1_current_state_rad', 'joint0_actions_rad', 'joint1_actions_rad']\n",
    "\n",
    "    X_a = pd.concat([X_a[\"prev_pos_x\"], X_a[\"prev_pos_y\"] , X_a[\"prev_pos_z\"],\n",
    "                X_a[\"prev_j_positions_0\"], X_a[\"prev_j_positions_1\"],\n",
    "                X_a[\"increments_0\"], X_a[\"increments_1\"]], \n",
    "                        axis=\"columns\")\n",
    "\n",
    "    X_a.columns = X_a_cols\n",
    "\n",
    "    y_a_cols = ['future_state_x', 'future_state_y', 'future_state_z', 'joint0_future_state_rad', 'joint1_future_state_rad']\n",
    "\n",
    "    y_a = pd.concat([y_a[\"post_pos_x\"], y_a[\"post_pos_y\"] , y_a[\"post_pos_z\"],\n",
    "                y_a[\"post_j_positions_0\"], y_a[\"post_j_positions_1\"]], \n",
    "                        axis=\"columns\")\n",
    "    y_a.columns = y_a_cols\n",
    "    return X_a, y_a\n",
    "\n",
    "def plot_history(history, loss):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel(f'{loss} Loss')\n",
    "  plt.plot(hist['epoch'], hist['loss'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_loss'],\n",
    "           label = 'Val Error')\n",
    "  #plt.ylim([0,0.1])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plot_pred_vs_test(test, pred):\n",
    "    columns = test.columns.tolist()\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    mse_list = []\n",
    "    perc_mse_list = []\n",
    "    for column_num in range(len(columns)):\n",
    "        plt.figure()\n",
    "        plt.scatter(test.iloc[:,column_num], pred.iloc[:,column_num], s=1)\n",
    "\n",
    "        #Getting important values\n",
    "        minim = min(test.iloc[:,column_num])\n",
    "        min_list.append(minim)\n",
    "        maxim = max(test.iloc[:,column_num])\n",
    "        max_list.append(maxim)\n",
    "        mse = mean_squared_error(test.iloc[:,column_num], pred.iloc[:,column_num])\n",
    "        mse_list.append(round(mse,6))\n",
    "        rang = (maxim-minim)\n",
    "        perc = mse/rang *100\n",
    "        perc_mse_list.append(round(perc,6))\n",
    "        title = columns[column_num] + \" - mse: \" + str(round(mse,6))\n",
    "        plt.title(title)\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predictions')\n",
    "        # Para tener una linea recta con la cual comparar los valores y que no altere\n",
    "        # los limites de la figura se grafican los valores reales con ellos mismos\n",
    "        plt.plot(test.iloc[:,column_num],test.iloc[:,column_num])\n",
    "        plt.grid()\n",
    "    data = {\"Perception\": columns, \"Max.\": max_list, \"Min.\": min_list, \"Mse\": mse_list, \"Mse perc.\": perc_mse_list}\n",
    "    data_df = pd.DataFrame(data)\n",
    "    return(data_df)\n",
    "\n",
    "def export2path(scene, model, scaler):\n",
    "    timestr = time.strftime(\"_%Y_%d_%m\")\n",
    "    models_path = \"models\\\\\" + timestr[1:]\n",
    "    if not os.path.exists(models_path):\n",
    "        os.mkdir(models_path)\n",
    "        print(f\"Directorio '{models_path}' ha sido creado.\")\n",
    "    else:\n",
    "        print(f\"El directorio '{models_path}' ya existe.\")\n",
    "    model.save(models_path + \"\\\\model_\" + scene + timestr + \".keras\")\n",
    "    with open(models_path + \"\\\\scaler_\" + scene + timestr + '.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(scene, model, scaler):\n",
    "    model.save(\"model_\" + scene + \".keras\")\n",
    "    with open(\"scaler_\" + scene + '.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    with open(\"model_\" + scene + '.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\OneDrive\\Imágenes\\Documentos\\GitHub\\EMERGE\\Morphology\\Multimodular\\modular02_learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28000, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene = \"modular02\"\n",
    "data_path = \"\"\n",
    "X, y =load_dataset(scene, data_path)\n",
    "\n",
    "# Se separan los datos de entrenmiento (80%) y los de prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y,\n",
    "test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling de los datos \n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 0.0397 - val_loss: 0.0023\n",
      "Epoch 2/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 6.0070e-04\n",
      "Epoch 3/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.8918e-04 - val_loss: 2.1891e-04\n",
      "Epoch 4/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.6212e-04 - val_loss: 1.1074e-04\n",
      "Epoch 5/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.0191e-04 - val_loss: 8.8626e-05\n",
      "Epoch 6/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 7.9960e-05 - val_loss: 6.8723e-05\n",
      "Epoch 7/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.7557e-05 - val_loss: 5.7086e-05\n",
      "Epoch 8/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.4054e-05 - val_loss: 5.8213e-05\n",
      "Epoch 9/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.8045e-05 - val_loss: 6.1431e-05\n",
      "Epoch 10/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.2245e-05 - val_loss: 5.1211e-05\n",
      "Epoch 11/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.9986e-05 - val_loss: 4.2273e-05\n",
      "Epoch 12/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.5235e-05 - val_loss: 4.0240e-05\n",
      "Epoch 13/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.4438e-05 - val_loss: 4.2259e-05\n",
      "Epoch 14/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.1404e-05 - val_loss: 3.5195e-05\n",
      "Epoch 15/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.7715e-05 - val_loss: 3.7233e-05\n",
      "Epoch 16/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.6223e-05 - val_loss: 2.9265e-05\n",
      "Epoch 17/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.4074e-05 - val_loss: 3.3683e-05\n",
      "Epoch 18/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.1972e-05 - val_loss: 2.8632e-05\n",
      "Epoch 19/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.1536e-05 - val_loss: 2.9817e-05\n",
      "Epoch 20/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.0704e-05 - val_loss: 2.7589e-05\n",
      "Epoch 21/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.0004e-05 - val_loss: 2.4640e-05\n",
      "Epoch 22/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.8714e-05 - val_loss: 2.8091e-05\n",
      "Epoch 23/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.9054e-05 - val_loss: 2.5953e-05\n",
      "Epoch 24/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.8557e-05 - val_loss: 2.3401e-05\n",
      "Epoch 25/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.7489e-05 - val_loss: 2.4300e-05\n"
     ]
    }
   ],
   "source": [
    "# Creacion y entrenamiento del modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de entrada (correspondiente al número de entradas)\n",
    "model.add(Dense(units=7, input_dim=7, activation='relu'))\n",
    "\n",
    "# Capas ocultas con 24 unidades cada una\n",
    "model.add(Dense(units=24, activation='relu'))\n",
    "\n",
    "# Capa de salida con 5 unidades (correspondiente al número de salidas) y activación lineal\n",
    "model.add(Dense(units=5, activation='linear'))\n",
    "\n",
    "# Compilar el modelo con una función de pérdida adecuada para regresión\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=1.0))\n",
    "\n",
    "# Supongamos que X_train y y_train son tus datos de entrenamiento\n",
    "history = model.fit(X_train_scaled, y_train, epochs=25, batch_size=32, validation_split=0.2)\n",
    "\n",
    "export(scene, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scene = \"modular02\"\n",
    "    data_path = \"\"\n",
    "    X, y =load_dataset(scene, data_path)\n",
    "    \n",
    "    # Se separan los datos de entrenmiento (80%) y los de prueba (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scaling de los datos \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled.shape\n",
    "\n",
    "    # Creacion y entrenamiento del modelo\n",
    "    model = Sequential()\n",
    "\n",
    "    # Capa de entrada (correspondiente al número de entradas)\n",
    "    model.add(Dense(units=7, input_dim=7, activation='relu'))\n",
    "\n",
    "    # Capas ocultas con 24 unidades cada una\n",
    "    model.add(Dense(units=24, activation='relu'))\n",
    "\n",
    "    # Capa de salida con 5 unidades (correspondiente al número de salidas) y activación lineal\n",
    "    model.add(Dense(units=5, activation='linear'))\n",
    "\n",
    "    # Compilar el modelo con una función de pérdida adecuada para regresión\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=1.0))\n",
    "\n",
    "    # Supongamos que X_train y y_train son tus datos de entrenamiento\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=25, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    export(scene, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\OneDrive\\Imágenes\\Documentos\\GitHub\\EMERGE\\Morphology\\Multimodular\\modular02_learning\n",
      "Epoch 1/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 0.0488 - val_loss: 0.0052\n",
      "Epoch 2/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0010\n",
      "Epoch 3/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 7.7461e-04 - val_loss: 5.9815e-04\n",
      "Epoch 4/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.9653e-04 - val_loss: 4.0054e-04\n",
      "Epoch 5/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 3.4559e-04 - val_loss: 2.9539e-04\n",
      "Epoch 6/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.4399e-04 - val_loss: 2.2541e-04\n",
      "Epoch 7/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.6814e-04 - val_loss: 1.4331e-04\n",
      "Epoch 8/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 1.2028e-04 - val_loss: 1.0965e-04\n",
      "Epoch 9/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 9.8072e-05 - val_loss: 9.0742e-05\n",
      "Epoch 10/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 8.5988e-05 - val_loss: 7.9937e-05\n",
      "Epoch 11/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 7.7055e-05 - val_loss: 7.2762e-05\n",
      "Epoch 12/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 7.2941e-05 - val_loss: 6.5498e-05\n",
      "Epoch 13/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.7073e-05 - val_loss: 7.1821e-05\n",
      "Epoch 14/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.3875e-05 - val_loss: 7.8089e-05\n",
      "Epoch 15/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 6.0068e-05 - val_loss: 5.7970e-05\n",
      "Epoch 16/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.7740e-05 - val_loss: 4.9287e-05\n",
      "Epoch 17/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.2909e-05 - val_loss: 5.2151e-05\n",
      "Epoch 18/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 5.0139e-05 - val_loss: 4.4700e-05\n",
      "Epoch 19/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.9284e-05 - val_loss: 4.8276e-05\n",
      "Epoch 20/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.7977e-05 - val_loss: 4.8086e-05\n",
      "Epoch 21/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.4762e-05 - val_loss: 4.9681e-05\n",
      "Epoch 22/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.5340e-05 - val_loss: 4.1623e-05\n",
      "Epoch 23/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.2220e-05 - val_loss: 4.1519e-05\n",
      "Epoch 24/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.1913e-05 - val_loss: 3.7042e-05\n",
      "Epoch 25/25\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 4.0211e-05 - val_loss: 3.7026e-05\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMERGE_Py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
