{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, pickle, time\n",
    "from zmqRemoteApi import RemoteAPIClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint as ri\n",
    "from random import uniform as ru\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.losses import Huber\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones previas de learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the training data into X,y datasets\n",
    "def load_dataset(scene_in = \"modular02a\", date = \"2023_15_11\"):\n",
    "    # Defining usefull variables\n",
    "    path = os.getcwd() + \"\\\\training_data\\\\\" + date\n",
    "    file_list = os.listdir(path)\n",
    "    scene_files_list = [item for item in file_list if scene_in in item and \"pkl\" in item]\n",
    "\n",
    "\n",
    "    file = open(path + \"\\\\\" + scene_files_list[0], \"rb\")\n",
    "    training_data = pickle.load(file)\n",
    "    training_df = pd.DataFrame(training_data)\n",
    "\n",
    "    #List to store the name for every joint data column\n",
    "    increments_columns = []\n",
    "    prev_j_positions_columns = []\n",
    "    post_j_positions_columns = []\n",
    "\n",
    "    #Creates the name for every column\n",
    "    num_joints = len(training_data[-1][\"increments\"])\n",
    "    for joint_n in range(num_joints):\n",
    "        #List to split inputs per joint\n",
    "        joint_inc_col_name = \"increments_\" + str(joint_n)\n",
    "        increments_columns.append(joint_inc_col_name)\n",
    "\n",
    "        prev_joint_pos_col_name = \"prev_j_positions_\" + str(joint_n)\n",
    "        prev_j_positions_columns.append(prev_joint_pos_col_name)\n",
    "\n",
    "        #List to split outputs per joint\n",
    "        post_joint_pos_col_name = \"post_j_positions_\" + str(joint_n)\n",
    "        post_j_positions_columns.append(post_joint_pos_col_name)\n",
    "\n",
    "\n",
    "    #Input columns per joint\n",
    "    increments_df = pd.DataFrame(training_df['increments'].to_list())\n",
    "    increments_df.columns = increments_columns\n",
    "\n",
    "\n",
    "    prev_j_positions_df = pd.DataFrame(training_df['prev_j_positions'].to_list())\n",
    "    prev_j_positions_df.columns = prev_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the X dataframe\n",
    "    X_df = pd.concat([increments_df, prev_j_positions_df, \n",
    "                    training_df[\"prev_pos_x\"], training_df[\"prev_pos_y\"], training_df[\"prev_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "\n",
    "    #Output columns per joint\n",
    "    post_j_positions_df = pd.DataFrame(training_df['post_j_positions'].to_list())\n",
    "    post_j_positions_df.columns = post_j_positions_columns\n",
    "\n",
    "\n",
    "    #Builds the y dataframe\n",
    "    y_df = pd.concat([post_j_positions_df, \n",
    "                    training_df[\"post_pos_x\"], training_df[\"post_pos_y\"], training_df[\"post_pos_z\"]], \n",
    "                    axis=\"columns\")\n",
    "\n",
    "    return(X_df,y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_X_y_02(X_a,y_a):\n",
    "    X_a_cols = ['current_state_x', 'current_state_y', 'current_state_z', 'joint0_current_state_rad', 'joint1_current_state_rad', 'joint0_actions_rad', 'joint1_actions_rad']\n",
    "\n",
    "    X_a = pd.concat([X_a[\"prev_pos_x\"], X_a[\"prev_pos_y\"] , X_a[\"prev_pos_z\"],\n",
    "                X_a[\"prev_j_positions_0\"], X_a[\"prev_j_positions_1\"],\n",
    "                X_a[\"increments_0\"], X_a[\"increments_1\"]], \n",
    "                        axis=\"columns\")\n",
    "\n",
    "    X_a.columns = X_a_cols\n",
    "\n",
    "    y_a_cols = ['future_state_x', 'future_state_y', 'future_state_z', 'joint0_future_state_rad', 'joint1_future_state_rad']\n",
    "\n",
    "    y_a = pd.concat([y_a[\"post_pos_x\"], y_a[\"post_pos_y\"] , y_a[\"post_pos_z\"],\n",
    "                y_a[\"post_j_positions_0\"], y_a[\"post_j_positions_1\"]], \n",
    "                        axis=\"columns\")\n",
    "    y_a.columns = y_a_cols\n",
    "    return X_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_X_y_03(X_a,y_a):\n",
    "    X_a_cols = ['current_state_x', 'current_state_y', 'current_state_z', 'joint0_current_state_rad', 'joint1_current_state_rad', 'joint2_current_state_rad'\n",
    "                , 'joint0_actions_rad', 'joint1_actions_rad', 'joint2_actions_rad']\n",
    "\n",
    "    X_a = pd.concat([X_a[\"prev_pos_x\"], X_a[\"prev_pos_y\"] , X_a[\"prev_pos_z\"],\n",
    "                X_a[\"prev_j_positions_0\"], X_a[\"prev_j_positions_1\"], X_a[\"prev_j_positions_2\"],\n",
    "                X_a[\"increments_0\"], X_a[\"increments_1\"], X_a[\"increments_2\"]], \n",
    "                        axis=\"columns\")\n",
    "\n",
    "    X_a.columns = X_a_cols\n",
    "\n",
    "    y_a_cols = ['future_state_x', 'future_state_y', 'future_state_z', 'joint0_future_state_rad', 'joint1_future_state_rad', 'joint2_future_state_rad']\n",
    "\n",
    "    y_a = pd.concat([y_a[\"post_pos_x\"], y_a[\"post_pos_y\"] , y_a[\"post_pos_z\"],\n",
    "                y_a[\"post_j_positions_0\"], y_a[\"post_j_positions_1\"], y_a[\"post_j_positions_2\"]], \n",
    "                        axis=\"columns\")\n",
    "    y_a.columns = y_a_cols\n",
    "    return X_a, y_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_test(test, pred):\n",
    "    columns = test.columns.tolist()\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    mse_list = []\n",
    "    perc_mse_list = []\n",
    "    for column_num in range(len(columns)):\n",
    "        plt.figure()\n",
    "        plt.scatter(test.iloc[:,column_num], pred.iloc[:,column_num])\n",
    "\n",
    "        #Getting important values\n",
    "        minim = min(pred.iloc[:,column_num])\n",
    "        min_list.append(minim)\n",
    "        maxim = max(pred.iloc[:,column_num])\n",
    "        max_list.append(maxim)\n",
    "        mse = mean_squared_error(test.iloc[:,column_num], pred.iloc[:,column_num])\n",
    "        mse_list.append(round(mse,6))\n",
    "        rang = (maxim-minim)\n",
    "        perc = mse/rang *100\n",
    "        perc_mse_list.append(round(perc,3))\n",
    "        title = columns[column_num] + \" - mse: \" + str(round(mse,6))\n",
    "        plt.title(title)\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predictions')\n",
    "        # Para tener una linea recta con la cual comparar los valores y que no altere\n",
    "        # los limites de la figura se grafican los valores reales con ellos mismos\n",
    "        plt.plot(test.iloc[:,column_num],test.iloc[:,column_num])\n",
    "        plt.grid()\n",
    "    data = {\"Perception\": columns, \"Max.\": max_list, \"Min.\": min_list, \"Mse\": mse_list, \"Mse perc.\": perc_mse_list}\n",
    "    data_df = pd.DataFrame(data)\n",
    "    return(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modular_scene():\n",
    "    def __init__(self, i_scene, i_date, i_model, i_scaler):\n",
    "        self.scene = i_scene\n",
    "        self.date = i_date\n",
    "        self.model = i_model\n",
    "        self.scaler = i_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['model_modular03_2023_15_11.keras'], ['scaler_modular03_2023_15_11.pkl'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene = \"modular03\"\n",
    "date = \"2023_15_11\"\n",
    "models_path = os.getcwd() + \"\\\\models\\\\\" + date\n",
    "file_list = os.listdir(models_path)\n",
    "model_files_list = [item for item in file_list if scene in item and \"model\" in item]\n",
    "scaler_files_list = [item for item in file_list if scene in item and \"scaler\" in item]\n",
    "model_files_list, scaler_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carlo\\OneDrive\\Im√°genes\\Documentos\\GitHub\\EMERGE\\Morphology\\Multimodular\\models\\2023_15_11\n"
     ]
    }
   ],
   "source": [
    "print(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moldels loaded:\n",
      "    0\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for model in model_files_list:\n",
    "    model_path = models_path + \"\\\\\" + model\n",
    "    loaded_model = load_model(model_path)\n",
    "    models[model_files_list.index(model)] = loaded_model\n",
    "print(\"Moldels loaded:\")\n",
    "for key in models.keys():\n",
    "    print(f\"    {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler object using pickle\n",
    "def load_scaler(file_name): #'standard_scaler.pkl'\n",
    "    with open(file_name, 'rb') as file:\n",
    "        model_scaler = pickle.load(file)\n",
    "    return model_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers loaded:\n",
      "    0\n"
     ]
    }
   ],
   "source": [
    "scalers = {}\n",
    "for scaler in scaler_files_list:\n",
    "    scaler_path = models_path + \"\\\\\" + scaler\n",
    "    loaded_scaler = load_scaler(scaler_path)\n",
    "    scalers[scaler_files_list.index(scaler)] = loaded_scaler\n",
    "print(\"Scalers loaded:\")\n",
    "for key in scalers.keys():\n",
    "    print(f\"    {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler object using pickle\n",
    "def load_scaler(file_name): #'standard_scaler.pkl'\n",
    "    with open(file_name, 'rb') as file:\n",
    "        model_scaler = pickle.load(file)\n",
    "    return model_scaler\n",
    "\n",
    "# Load the model and scaler of a scene\n",
    "def load_scene(scene = \"modular03\", date = \"2023_15_11\"):\n",
    "    models_path = os.getcwd() + \"\\\\models\\\\\" + date\n",
    "    file_list = os.listdir(models_path)\n",
    "    model_files_list = [item for item in file_list if scene in item and \"model\" in item]\n",
    "    scaler_files_list = [item for item in file_list if scene in item and \"scaler\" in item]\n",
    "    model_files_list, scaler_files_list\n",
    "\n",
    "    # Load models\n",
    "    models = {}\n",
    "    for model in model_files_list:\n",
    "        model_path = models_path + \"\\\\\" + model\n",
    "        loaded_model = load_model(model_path)\n",
    "        models[model_files_list.index(model)] = loaded_model\n",
    "    print(f\"Moldels loaded: {len(models.keys())}\")\n",
    "\n",
    "    # Load scalers\n",
    "    scalers = {}\n",
    "    for scaler in scaler_files_list:\n",
    "        scaler_path = models_path + \"\\\\\" + scaler\n",
    "        loaded_scaler = load_scaler(scaler_path)\n",
    "        scalers[scaler_files_list.index(scaler)] = loaded_scaler\n",
    "    print(f\"Scalers loaded: {len(scalers.keys())}\")\n",
    "    \n",
    "    latest_model = len(models.keys()) - 1\n",
    "    latest_scaler = len(scalers.keys()) - 1\n",
    "    \n",
    "    # Creates scene class with latest model/scaler\n",
    "    scene_class = modular_scene(scene, date, models[latest_model], scalers[latest_scaler])\n",
    "    print(scene_class.__dict__)\n",
    "    return scene_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moldels loaded: 1\n",
      "Scalers loaded: 1\n",
      "{'scene': 'modular03', 'date': '2023_15_11', 'model': <keras.src.engine.sequential.Sequential object at 0x0000020E27BC3950>, 'scaler': MinMaxScaler()}\n"
     ]
    }
   ],
   "source": [
    "modular = load_scene(\"modular03\", \"2023_15_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moldels loaded: 1\n",
      "Scalers loaded: 1\n",
      "{'scene': 'modular03', 'date': '2023_15_11', 'model': <keras.src.engine.sequential.Sequential object at 0x0000020E28E4EF10>, 'scaler': MinMaxScaler()}\n"
     ]
    }
   ],
   "source": [
    "scene = \"modular03\"\n",
    "date = \"2023_15_11\"\n",
    "modular = load_scene(scene, date)\n",
    "X, y =load_dataset(scene, date)\n",
    "if \"03\" in modular.scene:\n",
    "    X, y = adapt_X_y_03(X,y)\n",
    "else:\n",
    "    X, y = adapt_X_y_02(X,y)\n",
    "\n",
    "X_scaled = modular.scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = modular.model.predict(X_scaled)\n",
    "df_test_predictions = pd.DataFrame(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_test(y, df_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(scene, date):\n",
    "    modular = load_scene(scene, date)\n",
    "    X, y =load_dataset(scene, date)\n",
    "    if \"03\" in modular.scene:\n",
    "        X, y = adapt_X_y_03(X,y)\n",
    "    else:\n",
    "        X, y = adapt_X_y_02(X,y)\n",
    "\n",
    "    X_scaled = modular.scaler.transform(X)\n",
    "\n",
    "    test_predictions = modular.model.predict(X_scaled)\n",
    "    df_test_predictions = pd.DataFrame(test_predictions)\n",
    "    \n",
    "    plot_pred_vs_test(y, df_test_predictions)\n",
    "    return modular, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = \"modular03\"\n",
    "date = \"2023_15_11\"\n",
    "#test_model(scene, date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMERGE_Py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
